# -*- coding: utf-8 -*-
"""121450008_Muh Zaki A_Tugas PBA_GRU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XPEmUf3sK3HYWN7Y_i5ObwPe_qn3zezK

Models : GRU
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install dependencies
!pip install gensim
!pip install torch torchvision

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import re
from collections import Counter
import gensim
import numpy as np
from time import time

# Konfigurasi Langsung di Skrip
CONFIG = {
    "DATASET_PATH": "/content/drive/MyDrive/Dataset PBA/ag_news_csv",  # Path ke dataset
    "NUM_CLASSES": 4,
    "CLASSES": ['World', 'Sports', 'Business', 'Sci/Tech'],
    "EMBEDDING_TYPE": 'word2vec',  # Pilih 'word2vec' atau 'glove'
    "EMBEDDING_DIM": 100,
    "GLOVE_PATH": "/content/drive/MyDrive/Dataset PBA/dataset/glove.6B.100d.txt",
    "VOCAB_SIZE": 20000,
    "BATCH_SIZE": 64,
    "LEARNING_RATE": 0.01,
    "NUM_EPOCHS": 5,
    "DEVICE": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    "MAX_LENGTH": 100,  # Panjang maksimum teks
}

# Text Preprocessing dan Load GloVe
class TextPreprocessor:
    def __init__(self, config):
        self.config = config
        self.word_vectors = self.load_glove_embeddings(config['GLOVE_PATH'])

    @staticmethod
    def load_glove_embeddings(glove_path):
        """
        Memuat GloVe embeddings ke dalam dictionary.
        """
        if not os.path.exists(glove_path):
            raise FileNotFoundError(f"GloVe file not found at {glove_path}")

        embeddings = {}
        with open(glove_path, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                word = values[0]
                vector = np.asarray(values[1:], dtype='float32')
                embeddings[word] = vector
        print(f"GloVe embeddings loaded. Total words: {len(embeddings)}")
        return embeddings

    @staticmethod
    def clean_text(text):
        text = re.sub(r'[^a-zA-Z\s]', '', str(text).lower())
        return text

    def create_embedding_matrix(self, word_to_idx):
        """
        Membuat embedding matrix dari GloVe embeddings berdasarkan vocabulary.
        """
        embedding_dim = self.config['EMBEDDING_DIM']
        embedding_matrix = np.zeros((len(word_to_idx), embedding_dim))

        for word, idx in word_to_idx.items():
            if word in self.word_vectors:
                embedding_matrix[idx] = self.word_vectors[word]
            else:
                embedding_matrix[idx] = np.random.uniform(-0.25, 0.25, embedding_dim)

        print("Embedding matrix created with shape:", embedding_matrix.shape)
        return torch.tensor(embedding_matrix, dtype=torch.float32)

    @staticmethod
    def create_vocab(texts, max_size=20000):
        all_words = ' '.join(texts).split()
        word_counts = Counter(all_words)
        vocab = ['<pad>', '<unk>'] + [word for word, _ in word_counts.most_common(max_size-2)]
        word_to_idx = {word: idx for idx, word in enumerate(vocab)}
        return word_to_idx, vocab

# Dataset Class
class AGNewsDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, word_to_idx, max_length=100):
        texts = dataframe[1].apply(TextPreprocessor.clean_text)
        self.data = []
        for text, label in zip(texts, dataframe[0]):
            tokens = text.split()[:max_length]
            indexed_tokens = [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens]
            if len(indexed_tokens) < max_length:
                indexed_tokens += [word_to_idx['<pad>']] * (max_length - len(indexed_tokens))
            self.data.append({
                'text': torch.tensor(indexed_tokens, dtype=torch.long),
                'label': torch.tensor(int(label) - 1, dtype=torch.long)
            })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

class BiGRU(nn.Module):
    def __init__(self, n_classes, vocab_size, emb_size, embeddings, fine_tune, rnn_size, rnn_layers, dropout):
        super(BiGRU, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, emb_size)
        if embeddings is not None:
            self.embeddings.weight = nn.Parameter(embeddings, requires_grad=fine_tune)
        else:
            self.embeddings.weight.data.uniform_(-0.1, 0.1)

        self.BiGRU = nn.GRU(
            emb_size, rnn_size, num_layers=rnn_layers, bidirectional=True,
            dropout=(0 if rnn_layers == 1 else dropout), batch_first=True
        )
        self.fc = nn.Linear(2 * rnn_size, n_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        embeddings = self.dropout(self.embeddings(text))
        rnn_out, _ = self.BiGRU(embeddings)
        H = torch.mean(rnn_out, dim=1)
        scores = self.fc(self.dropout(H))
        return scores

# Training Function
def train_model(model, train_loader, config):
    device = config['DEVICE']
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])
    model.to(device)

    for epoch in range(config['NUM_EPOCHS']):
        model.train()
        total_loss, correct, total = 0, 0, 0
        for batch in train_loader:
            texts, labels = batch['text'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%")

def evaluate_model(model, data_loader, config):
    device = config['DEVICE']
    model.eval()  # Ubah mode ke evaluasi
    total, correct = 0, 0

    with torch.no_grad():  # Tidak perlu menghitung gradien saat evaluasi
        for batch in data_loader:
            texts, labels = batch['text'].to(device), batch['label'].to(device)
            outputs = model(texts)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    return accuracy

# Main Function
def main():
    # Load Dataset
    train_path = os.path.join(CONFIG['DATASET_PATH'], 'train.csv')
    train_df = pd.read_csv(train_path, header=None)

    # Preprocess dan Buat Embedding
    preprocessor = TextPreprocessor(CONFIG)
    word_to_idx, _ = preprocessor.create_vocab(train_df[1], CONFIG['VOCAB_SIZE'])
    embedding_matrix = preprocessor.create_embedding_matrix(word_to_idx)

    train_dataset = AGNewsDataset(train_df, word_to_idx, CONFIG)

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True)

    # Inisialisasi Model
    model = BiGRU(
        n_classes=CONFIG['NUM_CLASSES'],
        vocab_size=len(word_to_idx),
        emb_size=CONFIG['EMBEDDING_DIM'],
        embeddings=embedding_matrix,
        fine_tune=True,
        rnn_size=128,
        rnn_layers=2,
        dropout=0.3
    )

    # Latih Model
    train_model(model, train_loader, CONFIG)

if __name__ == "__main__":
    main()

def main():
    # Load Dataset
    train_path = os.path.join(CONFIG['DATASET_PATH'], 'train.csv')
    test_path = os.path.join(CONFIG['DATASET_PATH'], 'test.csv')
    train_df = pd.read_csv(train_path, header=None)
    test_df = pd.read_csv(test_path, header=None)

    preprocessor = TextPreprocessor(CONFIG)
    word_to_idx, _ = preprocessor.create_vocab(train_df[1], CONFIG['VOCAB_SIZE'])

    # Perhatikan argumen max_length diteruskan dengan nilai yang benar
    train_dataset = AGNewsDataset(train_df, word_to_idx, CONFIG['MAX_LENGTH'])
    test_dataset = AGNewsDataset(test_df, word_to_idx, CONFIG['MAX_LENGTH'])

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'])

    # Inisialisasi Model
    model = BiGRU(
        n_classes=CONFIG['NUM_CLASSES'],
        vocab_size=len(word_to_idx),
        emb_size=CONFIG['EMBEDDING_DIM'],
        embeddings=None,
        fine_tune=True,
        rnn_size=128,
        rnn_layers=2,
        dropout=0.3
    )

    # Latih Model
    train_model(model, train_loader, CONFIG)

    # Evaluasi Model
    train_accuracy = evaluate_model(model, train_loader, CONFIG)
    test_accuracy = evaluate_model(model, test_loader, CONFIG)

    print(f"Train Accuracy: {train_accuracy:.2f}%")
    print(f"Test Accuracy: {test_accuracy:.2f}%")

if __name__ == "__main__":
    main()

